{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "USER = !whoami\n",
    "USER = USER[0]\n",
    "OUTPUT_PATH_BASE = '/user/{USER}'.format(USER=USER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob_systems_test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob_systems_test.py\n",
    "#!/opt/anaconda/bin/python\n",
    "from mrjob.compat import jobconf_from_env\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "\n",
    "class MRCountLinesByFile(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        chars = len(line)\n",
    "        words = len(line.split())\n",
    "        lines = 1\n",
    "        self.increment_counter('Item Counters', 'Lines', lines)\n",
    "        self.increment_counter('Item Counters', 'Words', words)\n",
    "        self.increment_counter('Item Counters', 'Characters', chars)\n",
    "        yield \"chars\", chars\n",
    "        yield \"words\", words\n",
    "        yield \"lines\", lines\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield key, sum(values)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRCountLinesByFile.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stuff.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile stuff.txt\n",
    "Here I am in the altiscale\n",
    "Reading the jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/winegarj/.mrjob.conf\n",
      "Creating temp directory /tmp/mrjob_systems_test.winegarj.20171003.233225.513513\n",
      "Running step 1 of 1...\n",
      "Counters: 3\n",
      "\tItem Counters\n",
      "\t\tCharacters=54\n",
      "\t\tLines=2\n",
      "\t\tWords=10\n",
      "Counters: 3\n",
      "\tItem Counters\n",
      "\t\tCharacters=54\n",
      "\t\tLines=2\n",
      "\t\tWords=10\n",
      "Streaming final output from /tmp/mrjob_systems_test.winegarj.20171003.233225.513513/output...\n",
      "\"chars\"\t54\n",
      "\"lines\"\t2\n",
      "\"words\"\t10\n",
      "Removing temp directory /tmp/mrjob_systems_test.winegarj.20171003.233225.513513...\n"
     ]
    }
   ],
   "source": [
    "!python mrjob_systems_test.py stuff.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/10/03 23:32:27 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/winegarj/tests' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/winegarj/.Trash/Current\n",
      "Using configs in /home/winegarj/.mrjob.conf\n",
      "Using Hadoop version 2.7.3\n",
      "Creating temp directory /tmp/mrjob_systems_test.winegarj.20171003.233227.950756\n",
      "Copying local files to hdfs:///user/winegarj/tmp/mrjob/mrjob_systems_test.winegarj.20171003.233227.950756/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob7597047788158342884.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1506640654827_0406\n",
      "  Submitted application application_1506640654827_0406\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1506640654827_0406/\n",
      "  Running job: job_1506640654827_0406\n",
      "  Job job_1506640654827_0406 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1506640654827_0406 completed successfully\n",
      "  Output directory: hdfs:///user/winegarj/tests\n",
      "Counters: 52\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=83\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=32\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=74\n",
      "\t\tFILE: Number of bytes written=393102\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=439\n",
      "\t\tHDFS: Number of bytes written=32\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tItem Counters\n",
      "\t\tCharacters=54\n",
      "\t\tLines=2\n",
      "\t\tWords=10\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=8956416\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=8832000\n",
      "\t\tTotal time spent by all map tasks (ms)=5831\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=17493\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3450\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=17250\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=5831\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3450\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2490\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=242\n",
      "\t\tInput split bytes=356\n",
      "\t\tMap input records=2\n",
      "\t\tMap output bytes=62\n",
      "\t\tMap output materialized bytes=86\n",
      "\t\tMap output records=6\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1891622912\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce input records=6\n",
      "\t\tReduce output records=3\n",
      "\t\tReduce shuffle bytes=86\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=12\n",
      "\t\tTotal committed heap usage (bytes)=2282225664\n",
      "\t\tVirtual memory (bytes) snapshot=11408830464\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/winegarj/tmp/mrjob/mrjob_systems_test.winegarj.20171003.233227.950756...\n",
      "Removing temp directory /tmp/mrjob_systems_test.winegarj.20171003.233227.950756...\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = os.path.join(OUTPUT_PATH_BASE,'tests')\n",
    "!hadoop fs -rm -r {OUTPUT_PATH}\n",
    "!python mrjob_systems_test.py \\\n",
    "        -r hadoop stuff.txt \\\n",
    "        --output-dir={OUTPUT_PATH} \\\n",
    "        --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"chars\"\t54\r\n",
      "\"lines\"\t2\r\n",
      "\"words\"\t10\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -cat {OUTPUT_PATH}/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
